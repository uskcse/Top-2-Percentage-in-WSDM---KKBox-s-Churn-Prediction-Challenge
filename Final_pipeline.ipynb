{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import pickle\n",
    "import functools\n",
    "import gc\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from tqdm import tqdm\n",
    "import lightgbm as lgb\n",
    "import sklearn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################preprocessing member file##########################################################\n",
    "def preprocess_member(df,msno_file):\n",
    "    \"\"\"\n",
    "    Objective: Preprocess the membes detail file\n",
    "    \n",
    "    Input: Membership data frame, train or test data frame\n",
    "    \n",
    "    OutPut: preprocessed members deetail\n",
    "    \"\"\"\n",
    "    data = pd.merge(msno_file.reset_index(drop=True),df,on='msno',how='left')\n",
    "    (city_vector,gender_vector,registered_via_vector) = pickle.load(open(\"member.pickle\",\"rb\"))\n",
    "    #1. city\n",
    "    data_city = city_vector.transform(list(map(str,data['city'].values.astype(int))))\n",
    "    #2. bd\n",
    "    #fill the nan value with -1 in age data\n",
    "    data['bd'] = data['bd'].fillna(-1)\n",
    "    # remove all the ages greater than 85 and lesser than 10 in train data\n",
    "    data['bd'] = list(map(lambda x: -1 if x<10 or x>85 else x,data.bd))\n",
    "    #3. gender\n",
    "    data_gender = gender_vector.transform(list(map(str,data['gender'].values)))\n",
    "    #4. registered_via\n",
    "    data_registered_via = registered_via_vector.transform(list(map(str,data['registered_via'].values.astype(int))))\n",
    "    #5. Registration init time \n",
    "    data['registration_init_time'] = data['registration_init_time'].fillna(-1)\n",
    "    #stacking\n",
    "    data = np.hstack((data[['bd','registration_init_time']].to_numpy(),data_city.todense(),data_gender.todense(),data_registered_via.todense()))   \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_feature(files,msno_file,name,last_day,month,days):\n",
    "    features = ['num_25', 'num_50', 'num_75', 'num_985', 'num_100',\n",
    "          'num_unq','total_secs']\n",
    "    if len(files)>5:\n",
    "        f = [files[:5],files[5:]]\n",
    "    else:\n",
    "        f = [files]\n",
    "    merge = []\n",
    "    for c in range(0,len(f)):\n",
    "        #get features from log files from 2016\n",
    "        merge_files = []\n",
    "        for i in tqdm(features[:-1]):\n",
    "            #load all the files we made from last iteration. But only loading pariticular columns to avoid memor constraints\n",
    "            df = pd.concat([pd.read_csv(f,usecols=['msno',i,'date']) for f in f[c]])\n",
    "            #to avoid leakage problem we only slecing data untill 2016\n",
    "            df = pd.merge(pd.read_csv(msno_file).drop(['Unnamed: 0','is_churn'],axis=1),df,on='msno',how='left')\n",
    "            gc.collect()\n",
    "            df=df[df['date'].le(last_day)][['msno',i]]\n",
    "            #creating sum,mean,std of a msno feature\n",
    "            df=df.groupby('msno').agg({i:['sum','mean']})\n",
    "            #rename the column name as ater group and aggregation names might be diffrent\n",
    "            df.columns = df.columns = [\"_\".join(x) for x in df.columns.ravel()]\n",
    "            #export our feature as csv\n",
    "            df.to_csv(str(i)+'_'+str(c)+'_'+name+\".csv\")\n",
    "            #clear the ram for next iteration\n",
    "            merge_files.append(str(i)+'_'+str(c)+'_'+name+\".csv\")\n",
    "            del df\n",
    "            gc.collect()\n",
    "        ##################################################################################################################\n",
    "        #load all the files we made from last iteration. But only loading pariticular columns to avoid memory  constraints\n",
    "        df = pd.concat([pd.read_csv(f,usecols=['msno','total_secs','date']) for f in f[c]])\n",
    "        #to avoid leakage problem we only slecing data untill 2016\n",
    "        df = pd.merge(pd.read_csv(msno_file).drop(['Unnamed: 0'],axis=1),df,on='msno',how='left')\n",
    "        gc.collect()\n",
    "        df=df[df['date'].le(last_day)][['msno','total_secs']]\n",
    "        #creating sum,mean,std of a msno feature\n",
    "        df=df.groupby('msno').agg({'total_secs':['sum','mean','count']})\n",
    "        #rename the column name as ater group and aggregation names might be diffrent\n",
    "        df.columns = df.columns = [\"_\".join(x) for x in df.columns.ravel()]\n",
    "        #export our feature as csv\n",
    "        df.to_csv('total_secs_'+str(c)+'_'+name+\".csv\")\n",
    "        merge_files.append('total_secs_'+str(c)+'_'+name+\".csv\")\n",
    "        #clear the ram for next iteration\n",
    "        del df\n",
    "        gc.collect()\n",
    "        merge.append(merge_files)\n",
    "    final_merge = []\n",
    "    for i in range(len(features)):\n",
    "        df = pd.concat([pd.read_csv(f[i]).rename(columns = {'msno_':'msno'}) for f in merge])\n",
    "        val =dict()\n",
    "        for j in list(df.columns):\n",
    "            if 'sum' in j or 'count' in j:\n",
    "                val[j]=['sum']\n",
    "            elif 'mean' in j:\n",
    "                val[j]=['mean']\n",
    "        df = df.groupby('msno').agg(val)\n",
    "        df.columns = df.columns = [x[0] for x in df.columns.ravel()]\n",
    "        df = df.rename(columns = {'msno_':'msno'})\n",
    "        df.to_csv(features[i]+'_{}.csv'.format(name))\n",
    "        final_merge.append(features[i]+'_{}.csv'.format(name))\n",
    "    data_frames = [pd.read_csv(msno_file).drop(['Unnamed: 0','is_churn'],axis=1)]\n",
    "    data_frames += [pd.read_csv(f) for f in final_merge]\n",
    "    df_merged = functools.reduce(lambda  left,right: pd.merge(left,right,on=['msno'],how='left'), data_frames)\n",
    "    df_merged.to_csv(\"all_log_{}.csv\".format(name))\n",
    "    del df_merged\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def last_month_features(files,msno_file,name,date,f_name):\n",
    "    #sum_feature(files,msno_file,name,last_day,date,days)\n",
    "    #last date features from log file\n",
    "    features = ['num_25', 'num_50', 'num_75', 'num_985', 'num_100',\n",
    "          'num_unq','total_secs']\n",
    "    merge_files = []\n",
    "    for i in tqdm(features[:-1]):\n",
    "        #load all the files we made from last iteration. But only loading pariticular columns to avoid memor constraints\n",
    "        temp = []\n",
    "        for f in files:\n",
    "            df = pd.read_csv(f,usecols=['msno',i,'date'])\n",
    "            df = df[df['date'].between(date[0],date[1])][['msno',i]]\n",
    "            temp.append(df)\n",
    "            gc.collect()\n",
    "        df = pd.concat(temp)\n",
    "        df = pd.merge(pd.read_csv(msno_file).drop(['Unnamed: 0','is_churn'],axis=1),df,on='msno',how='left')\n",
    "        #creating sum,mean,std of a msno feature\n",
    "        df=df.groupby('msno').agg({i:['sum','mean','std']}).reset_index()\n",
    "        #rename the column name as ater group and aggregation names might be diffrent\n",
    "        df.columns = df.columns = [\"_\".join(x) for x in df.columns.ravel()]\n",
    "        df.columns = df.columns = [f_name+\"_\"+x for x in df.columns]\n",
    "        df = df.rename(columns = {f_name+\"_msno_\":'msno'})\n",
    "        #export our feature as csv\n",
    "        df.to_csv(f_name+'_'+str(i)+'_'+name+\".csv\")\n",
    "        merge_files.append(f_name+'_'+str(i)+'_'+name+\".csv\")\n",
    "        #clear the ram for next iteration\n",
    "        del df\n",
    "        gc.collect()    \n",
    "\n",
    "    #load all the files we made from last iteration. But only loading pariticular columns to avoid memor constraints\n",
    "    temp = []\n",
    "    for f in files:\n",
    "        df = pd.read_csv(f,usecols=['msno','total_secs','date'])\n",
    "        df = df[df['date'].between(date[0],date[1])][['msno','total_secs']]\n",
    "        temp.append(df)\n",
    "        gc.collect()\n",
    "    df = pd.concat(temp)\n",
    "    df = pd.merge(pd.read_csv(msno_file).drop(['Unnamed: 0','is_churn'],axis=1),df,on='msno',how='left')\n",
    "    #creating sum,mean,std of a msno feature\n",
    "    df=df.groupby('msno').agg({'total_secs':['sum','mean','std','count']}).reset_index()\n",
    "    #rename the column name as ater group and aggregation names might be diffrent\n",
    "    df.columns = df.columns = [\"_\".join(x) for x in df.columns.ravel()]\n",
    "    df.columns = df.columns = [f_name+\"_\"+x for x in df.columns]\n",
    "    df = df.rename(columns = {f_name+\"_msno_\":'msno'})\n",
    "    #export our feature as csv\n",
    "    df.to_csv(f_name+'_total_secs_'+name+\".csv\")\n",
    "    merge_files.append(f_name+'_total_secs_'+name+\".csv\")\n",
    "    #clear the ram for next iteration\n",
    "    del df\n",
    "    gc.collect()   \n",
    "    data_frames = [pd.read_csv(msno_file).drop(['Unnamed: 0','is_churn'],axis=1)]\n",
    "    data_frames += [pd.read_csv(f).drop(['Unnamed: 0'],axis=1) for f in merge_files]    \n",
    "    df_merged = functools.reduce(lambda  left,right: pd.merge(left,right,on=['msno'],how='left'), data_frames)\n",
    "    df_merged.to_csv(\"{}_log_{}.csv\".format(f_name,name))\n",
    "    del df_merged\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Final_day_features(files,msno_file,name,date,f_name):\n",
    "    #sum_feature(files,msno_file,name,last_day,date,days)\n",
    "    #last date features from log file\n",
    "    features = ['num_25', 'num_50', 'num_75', 'num_985', 'num_100',\n",
    "          'num_unq','total_secs']\n",
    "    merge_files = []\n",
    "    for i in tqdm(features):\n",
    "        #load all the files we made from last iteration. But only loading pariticular columns to avoid memor constraints\n",
    "        temp = []\n",
    "        for f in files:\n",
    "            df = pd.read_csv(f,usecols=['msno',i,'date'])\n",
    "            df = df[df['date']==date][['msno',i]]\n",
    "            temp.append(df)\n",
    "            gc.collect()\n",
    "        df = pd.concat(temp)\n",
    "        df = pd.merge(pd.read_csv(msno_file).drop(['Unnamed: 0','is_churn'],axis=1),df,on='msno',how='left')\n",
    "        #rename the column name as ater group and aggregation names might be diffrent\n",
    "        df.columns = df.columns = [f_name+\"_\"+x for x in df.columns]\n",
    "        df = df.rename(columns = {f_name+\"_msno\":'msno'})\n",
    "        #export our feature as csv\n",
    "        df.to_csv(f_name+'_'+str(i)+'_'+name+\".csv\")\n",
    "        merge_files.append(f_name+'_'+str(i)+'_'+name+\".csv\")\n",
    "        #clear the ram for next iteration\n",
    "        del df\n",
    "        gc.collect()    \n",
    "    data_frames = [pd.read_csv(msno_file).drop(['Unnamed: 0','is_churn'],axis=1)]\n",
    "    data_frames += [pd.read_csv(f).drop(['Unnamed: 0'],axis=1) for f in merge_files]    \n",
    "    df_merged = functools.reduce(lambda  left,right: pd.merge(left,right,on=['msno'],how='left'), data_frames)\n",
    "    df_merged = df_merged.drop_duplicates(subset=['msno'], keep=\"first\", inplace=False)\n",
    "    df_merged.to_csv(\"{}_log_{}.csv\".format(f_name,name))\n",
    "    del df_merged\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_log(files,msno_file,name,last_day,month,days):\n",
    "    \"\"\"\n",
    "    Objective: Preprocess the user_log file and create a sum,avg,std features for given columns\n",
    "    \n",
    "    Input: file:transaction_filename,msno_file=train or test file name,\n",
    "    name:name of the file we preprocess,last_day:transaction last day,month:transaction last month,lastday:last 15 days\n",
    "    \n",
    "    OutPut: preprocessed members deetail\n",
    "    \"\"\"\n",
    "    sum_feature(files,msno_file,name,last_day,month,days)\n",
    "    last_month_features(files,msno_file,name,month,'last_month')\n",
    "    last_month_features(files,msno_file,name,days,'last_15days')\n",
    "    Final_day_features(files,msno_file,name,last_day,'final_day')\n",
    "#     merge_files = [msno_file,'Processed/all_log_{}.csv'.format(name),\"Processed/last_month_log_{}.csv\".format(name),\"Processed/last_15days_log_{}.csv\".format(name),\"Processed/final_day_log_{}.csv\".format(name)]\n",
    "#     data_frames = [pd.read_csv(f).drop(['Unnamed: 0'],axis=1) for f in merge_files]    \n",
    "#     df_merged = functools.reduce(lambda  left,right: pd.merge(left,right,on=['msno'],how='left'), data_frames)\n",
    "#     df_merged.to_csv(\"Final/Final_log_{}.csv\".format(name))\n",
    "#     del df_merged\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reference https://github.com/G-pravin-shankar/Kaggle-Top-4-percent-Solution-WSDM-KKBOX-Churn-Prediction/\n",
    "def last_5_calc(msno_file,msno,last_5,last =-1):\n",
    "    \"\"\"\n",
    "    Objective: Compute last five churn list\n",
    "    Input : datafram with transaction_date and membership_expire date, size of train set, msno of trainset\n",
    "    Output: return dictionary with msno key and last five churn details as value list\n",
    "    \"\"\"\n",
    "    dic =dict(zip(pd.read_csv(msno_file)['msno'],np.arange(len(pd.read_csv(msno_file)['msno']))))\n",
    "    Last_5_values = np.full((pd.read_csv(msno_file).shape[0], 5), last) #default value -1 meaning no transaction. Also index 5=last1 churn(becasue of asecending order)\n",
    "    count=0\n",
    "    index=-1\n",
    "    for i in range(msno.shape[0]):\n",
    "        ind=dic[msno[i]]\n",
    "        if ind==index:\n",
    "            count+=1\n",
    "        else: #arrange values in right position and change 'count' to 0\n",
    "            if count==3:\n",
    "                Last_5_values[index][4] = Last_5_values[index][3]\n",
    "                Last_5_values[index][3] = Last_5_values[index][2]\n",
    "                Last_5_values[index][2] = Last_5_values[index][1]\n",
    "                Last_5_values[index][1] = Last_5_values[index][0]\n",
    "                Last_5_values[index][0]=-1\n",
    "            elif count==2:\n",
    "                Last_5_values[index][4] = Last_5_values[index][2]\n",
    "                Last_5_values[index][3] = Last_5_values[index][1]\n",
    "                Last_5_values[index][2] = Last_5_values[index][0]\n",
    "                Last_5_values[index][1]= -1\n",
    "                Last_5_values[index][0]= -1\n",
    "            elif count==1:\n",
    "                Last_5_values[index][4] = Last_5_values[index][1]\n",
    "                Last_5_values[index][3] = Last_5_values[index][0]\n",
    "                Last_5_values[index][1] = -1\n",
    "                Last_5_values[index][0] = -1\n",
    "            elif count==0:\n",
    "                Last_5_values[index][4] = Last_5_values[index][0]\n",
    "                Last_5_values[index][0] = -1\n",
    "            count=0\n",
    "        index=ind\n",
    "        Last_5_values[index][count] = last_5[i]\n",
    "    return Last_5_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Transaction_preprocess(files, msno_file, last_date ,name, date):\n",
    "    \"\"\"\n",
    "    Objective : Preprocess the transaction file and save features as csv\n",
    "    Input     : File names of transaction file, File name of msno file, name of set,last transactio date, last membership expire date\n",
    "    Output    : save all the preprocessed features as csv\n",
    "    \"\"\"\n",
    "    train = pd.read_csv(msno_file).drop(['Unnamed: 0'],axis=1)\n",
    "    trans = pd.concat([pd.read_csv(f) for f in files])\n",
    "    trans_before_last_date = trans[trans.transaction_date.le(last_date)]\n",
    "    trans_before_last_date = trans_before_last_date[trans_before_last_date['membership_expire_date'].le(date[1])]\n",
    "    trans_before_last_date = trans_before_last_date[trans_before_last_date.msno.isin(train['msno'])]\n",
    "    trans_before_last_date = trans_before_last_date.sort_values(by=['msno', 'transaction_date'],ascending=[True,True])\n",
    "    #1. total number of cancel feature\n",
    "    print(\"1. total number of cancel feature\")\n",
    "    total_cancel = trans_before_last_date.groupby(['msno'], as_index=False)['is_cancel'].sum()\n",
    "    total_cancel = pd.merge(train, total_cancel, on='msno',how='left')\n",
    "    np.save('{}_total_num_cancel.npy'.format(name),list(total_cancel['is_cancel']))\n",
    "    #2. Last Before transaction features\n",
    "    print(\"2. Last Before transaction features\")\n",
    "    #https://stackoverflow.com/a/19464054\n",
    "    index = trans_before_last_date[['msno','membership_expire_date','transaction_date','is_cancel']].shift(1) != trans_before_last_date[['msno','membership_expire_date','transaction_date','is_cancel']]\n",
    "    trans_before_last_date = trans_before_last_date[index.apply(lambda x: (x[0] or x[1]) or (x[2] or x[3]),axis=1)]\n",
    "    #https://stackoverflow.com/a/38965036\n",
    "    las_before_transaction = trans_before_last_date.groupby(['msno']).tail(2).drop_duplicates(['msno'])\n",
    "    las_before_transaction = pd.merge(train, las_before_transaction, on='msno',how='left')\n",
    "    only_one_trans = trans_before_last_date.msno.value_counts(sort=False)\n",
    "    index  = np.where(only_one_trans == 1)[0]\n",
    "    one_trans = list(map(lambda x:only_one_trans.index[x],index))\n",
    "    las_before_transaction[las_before_transaction['msno'].isin(one_trans)] = np.nan\n",
    "    las_before_transaction=las_before_transaction.rename(columns={'payment_method_id': 'Last_before_payment_method_id', 'payment_plan_days': 'Last_before_payment_plan_days',\\\n",
    "                                                         'plan_list_price':'Last_before_plan_list_price','actual_amount_paid':'Last_before_actual_amount_paid',\\\n",
    "                                                         'is_auto_renew':'Last_before_is_auto_renew','transaction_date':'Last_before_transaction_date',\\\n",
    "                                                         'membership_expire_date':'Last_before_membership_expire_date','is_cancel':'Last_before_is_cancel'})\n",
    "    las_before_transaction =  las_before_transaction.drop(['msno'], axis=1)\n",
    "    las_before_transaction.to_csv('{}_last_before_transact_features_msno_order'.format(name))\n",
    "    #3. Final transaction feature\n",
    "    print(\"3. Final transaction feature\")\n",
    "    final_transaction = trans_before_last_date.groupby('msno').tail(1)\n",
    "    final_transaction = pd.merge(train.msno, final_transaction, on='msno',how='left')\n",
    "    final_transaction[final_transaction['membership_expire_date']<date[0]]\n",
    "    final_transaction.to_csv('{}_final_transact.csv'.format(name))\n",
    "    # Last 5 Churn and total churn and not churn rate\n",
    "    print(\" Last 5 Churn and total churn and not churn rate\")\n",
    "    trans_before_date_up = trans_before_last_date\n",
    "    trans_before_date_up['Transaction_date_moved_up'] = list(trans_before_last_date['transaction_date'])[1:]+[date[0]]\n",
    "    churn_rows = trans_before_date_up[['membership_expire_date','Transaction_date_moved_up']]\n",
    "    churn = churn_rows.apply(lambda x: (((x[1]//10000*365)+(x[1]%10000//100*30.4167)+(x[1]%100))-((x[0]//10000*365)+(x[0]%10000//100*30.4167)+(x[0]%100)))<31, axis=1)\n",
    "    last_not_churn = list(churn)\n",
    "    np.save('churn_or_not_{}.npy'.format(name),last_not_churn)\n",
    "    last_not_churn = np.load('churn_or_not_{}.npy'.format(name))\n",
    "    trans_before_date_up['churn_or_not'] = last_not_churn\n",
    "    ind=trans_before_date_up.groupby(['msno']).tail(1)\n",
    "    #trans_before_date_up = trans_before_date_up[~trans_before_date_up.isin(ind)] its not working when we have duplicates in test\n",
    "    #https://stackoverflow.com/questions/38681340/how-to-remove-common-rows-in-two-dataframes-in-pandas#:~:text=4%20Answers&text=You%20can%20use%20pandas.,the%20duplicated%20rows%20in%20them.\n",
    "\n",
    "    df = trans_before_date_up.drop_duplicates().merge(ind.drop_duplicates(), on=ind.columns.to_list(), \n",
    "                   how='left', indicator=True)\n",
    "    trans_before_date_up = df.loc[df._merge=='left_only',df.columns!='_merge']\n",
    "    #4. Total_churn_and_not_churn_count:\n",
    "    print(\"4. Total_churn_and_not_churn_count:\")\n",
    "    trans_before_date_up_ = trans_before_date_up.dropna()[['msno','churn_or_not']]\n",
    "    msno = trans_before_date_up_['msno'].to_numpy()\n",
    "    churn_or_not = trans_before_date_up_['churn_or_not'].to_numpy()\n",
    "    dic =dict(zip(pd.read_csv(msno_file)['msno'],np.arange(len(pd.read_csv(msno_file)['msno']))))\n",
    "    total_churn_or_not_count=np.full((pd.read_csv(msno_file).shape[0],2), 0)\n",
    "    for i in tqdm(range(trans_before_date_up_.shape[0])):\n",
    "        index = dic[msno[i]]\n",
    "        if churn_or_not[i]==1: #not churn\n",
    "            total_churn_or_not_count[index][1]+=1\n",
    "        else: #churn\n",
    "            total_churn_or_not_count[index][0]+=1\n",
    "    np.save('total_churn_or_not_count_{}'.format(name),total_churn_or_not_count)\n",
    "    #5. Last_5_not_churns:\n",
    "    print(\"5. Last_5_not_churns:\")\n",
    "    trans_before_date_up = trans_before_date_up.groupby(['msno']).tail(5)\n",
    "    trans_before_date_up = trans_before_date_up.dropna()\n",
    "    trans_before_date_up = trans_before_date_up[['msno','churn_or_not']]\n",
    "    msno = trans_before_date_up['msno'].to_numpy()\n",
    "    churn_or_not = trans_before_date_up['churn_or_not'].to_numpy()\n",
    "    Last_5_churn_or_not = last_5_calc(msno_file,msno,churn_or_not)\n",
    "    np.save('Last_5_churn_or_not_{}'.format(name),Last_5_churn_or_not)\n",
    "    # additinal feature genration\n",
    "    print(\"additinal feature genration\")\n",
    "    transactions = pd.concat([pd.read_csv(f) for f in files])\n",
    "    trans_before_last_date = transactions[transactions['transaction_date'].le(last_date)]\n",
    "    trans_before_last_date = trans_before_last_date[trans_before_last_date['msno'].isin(train['msno'])]\n",
    "    trans_before_last_date = trans_before_last_date.sort_values(by=['msno', 'transaction_date'],ascending=[True,True])\n",
    "    trans_before_last_date = trans_before_last_date[trans_before_last_date['is_cancel']==0]\n",
    "    #6.Discount feature \n",
    "    print(\"6. Discount feature \")\n",
    "    trans_before_last_date['discount'] = trans_before_last_date['plan_list_price']-trans_before_last_date['actual_amount_paid']\n",
    "    trans_before_last_date['discount'] = list(map(lambda x: -1 if x<0 else x,tqdm(trans_before_last_date['discount'])))\n",
    "    #7pay per day\n",
    "    print(\"7. pay per day\")\n",
    "    trans_before_last_date['per_day_price'] = trans_before_last_date['plan_list_price']/trans_before_last_date['payment_plan_days']\n",
    "    #impute missing values with -1\n",
    "    trans_before_last_date['per_day_price'] = trans_before_last_date['per_day_price'].fillna(-1)\n",
    "    index = trans_before_last_date[['msno','membership_expire_date']].shift(1) !=trans_before_last_date[['msno','membership_expire_date']]\n",
    "    trans_before_last_date = trans_before_last_date[index.apply(lambda x: (x[0] or x[1]),axis = 1)]\n",
    "    #8. Sum_count_and_average_features:\n",
    "    print(\"8. Sum_count_and_average_features:\")\n",
    "    sum_feature = trans_before_last_date.groupby(['msno'], as_index=False)['actual_amount_paid','payment_plan_days','discount','per_day_price'].sum()\n",
    "    count_feature = trans_before_last_date.groupby(['msno'], as_index=False)['actual_amount_paid','payment_plan_days','discount','per_day_price'].count()\n",
    "    count_feature[['actual_amount_paid_avg','payment_plan_days_avg','discount_avg','per_day_price_avg']] = sum_feature[['actual_amount_paid', 'payment_plan_days', 'discount','per_day_price']]/count_feature[['actual_amount_paid','payment_plan_days','discount','per_day_price']]\n",
    "    count_feature = count_feature[['msno','actual_amount_paid','actual_amount_paid_avg','payment_plan_days_avg','discount_avg','per_day_price_avg']]\n",
    "    count_feature.columns=['msno','count','actual_amount_paid_avg','payment_plan_days_avg','discount_avg','per_day_price_avg']\n",
    "    count_feature['discount_avg']=list(map(lambda x: -1 if x<0 else x,tqdm(count_feature['discount_avg'])))\n",
    "    count_feature['per_day_price_avg']=list(map(lambda x: -1 if x<0 else x,tqdm(count_feature['per_day_price_avg'])))\n",
    "    sum_feature = sum_feature.merge(count_feature,on='msno')\n",
    "    sum_feature = pd.DataFrame(train['msno'],columns=['msno']).merge(sum_feature,on='msno',how='left')\n",
    "    sum_feature = sum_feature.fillna(-1)\n",
    "    sum_feature = sum_feature.drop(['msno'],axis=1)\n",
    "    np.save('transact_sum_count_avg_features_{}'.format(name),np.array(sum_feature))\n",
    "    #last five features\n",
    "    print(\"9. slast five features\")\n",
    "    trans_before_last_date_5 = trans_before_last_date.groupby(['msno']).tail(5)\n",
    "    msno=trans_before_last_date_5['msno'].to_numpy()\n",
    "    Last_5_discount = trans_before_last_date_5['discount'].to_numpy()\n",
    "    Last_5_payment_plan_days=trans_before_last_date_5['payment_plan_days'].to_numpy()\n",
    "    Last_5_actual_amount_paid=trans_before_last_date_5['actual_amount_paid'].to_numpy()\n",
    "    Last_5_per_day_price=trans_before_last_date_5['per_day_price'].to_numpy()\n",
    "    Last_5_discount = last_5_calc(msno_file,msno,Last_5_discount)\n",
    "    np.save('Last_5_discount_{}'.format(name),Last_5_discount)\n",
    "    Last_5_payment_plan_days = last_5_calc(msno_file,msno,Last_5_payment_plan_days)\n",
    "    np.save('Last_5_payment_plan_days_{}'.format(name),Last_5_payment_plan_days)\n",
    "    Last_5_actual_amount_paid = last_5_calc(msno_file,msno,Last_5_actual_amount_paid)\n",
    "    np.save('Last_5_actual_amount_paid_{}'.format(name),Last_5_actual_amount_paid)\n",
    "    Last_5_per_day_price = last_5_calc(msno_file,msno,Last_5_per_day_price,last =-1.0)\n",
    "    np.save('Last_5_per_day_price_{}'.format(name),Last_5_per_day_price)\n",
    "    #Final Transaction Additional Features\n",
    "    print(\"10. Final Transaction Additional Features\")\n",
    "    final_trans = pd.read_csv(\"{}_final_transact.csv\".format(name))\n",
    "    #https://stackoverflow.com/questions/27506367/python-pandas-integer-yyyymmdd-to-datetime\n",
    "    final_trans['transaction_date'] = pd.to_datetime(final_trans['transaction_date'].astype(str), format='%Y%m%d')\n",
    "    final_trans['membership_expire_date']=pd.to_datetime(final_trans['membership_expire_date'].astype(str), format='%Y%m%d')\n",
    "    #number of days from transaction date to membership expire date\n",
    "    print(\"11. number of days from transaction date to membership expire date\")\n",
    "    Expiry_Trans_interval = final_trans['membership_expire_date']-final_trans['transaction_date']\n",
    "    #https://stackoverflow.com/a/18215499\n",
    "    Expiry_Trans_interval = Expiry_Trans_interval/np.timedelta64(1, 'D')\n",
    "    #impute -1 for missing values\n",
    "    Expiry_Trans_interval[Expiry_Trans_interval<0]=-1\n",
    "    np.save('Expiry_Transaction_interval_{}'.format(name),Expiry_Trans_interval)\n",
    "    #Days_since_final_transact:\n",
    "    print(\"12. Days_since_final_transact:\")\n",
    "    Days_since_final_trans = pd.to_datetime(date[0],format='%Y%m%d') - final_trans['transaction_date']\n",
    "    Days_since_final_trans = Days_since_final_trans/np.timedelta64(1, 'D')\n",
    "    np.save('Days_since_final_transact_{}'.format(name),Days_since_final_trans)\n",
    "    #Days left\n",
    "    print(\"13. Days left\")\n",
    "    days_Left = final_trans['membership_expire_date']-pd.to_datetime(last_date,format='%Y%m%d')\n",
    "    days_Left = days_Left/np.timedelta64(1, 'D')\n",
    "    np.save('days_Left_{}'.format(name),days_Left)\n",
    "    #Payment_plan_days_subract_interval:\n",
    "    print(\"14. Payment_plan_days_subract_interval:\")\n",
    "    final_trans['Expiry_Transaction_interval']=Expiry_Trans_interval\n",
    "    Expiry_Trans_interval-final_trans['payment_plan_days']\n",
    "    Payment_plan_days_subract_interval = Expiry_Trans_interval - final_trans['payment_plan_days']\n",
    "    np.save('Payment_plan_days_subract_interval_{}'.format(name),Payment_plan_days_subract_interval)\n",
    "    # https://github.com/G-pravin-shankar/Kaggle-Top-4-percent-Solution-WSDM-KKBOX-Churn-Prediction/blob/master/Transcation_based_features_additional_train.ipynb\n",
    "    Days_Left=np.load('days_Left_{}.npy'.format(name))\n",
    "    Days_since_final_transact=np.load('Days_since_final_transact_{}.npy'.format(name))\n",
    "    Expiry_Transaction_interval=np.load('Expiry_Transaction_interval_{}.npy'.format(name))\n",
    "    Payment_plan_days_subract_interval=np.load('Payment_plan_days_subract_interval_{}.npy'.format(name))\n",
    "    Final_Add=np.hstack((Days_Left.reshape(-1,1),Days_since_final_transact.reshape(-1,1),Expiry_Transaction_interval.reshape(-1,1),Payment_plan_days_subract_interval.reshape(-1,1)))\n",
    "    np.save('Final_Add_{}.npy'.format(name),Final_Add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(name,date):\n",
    "    #Load all log preprocessed data\n",
    "    final_day_log = pd.read_csv(\"final_day_log_{}.csv\".format(name)).drop([i for i in pd.read_csv(\"final_day_log_{}.csv\".format(name)).columns if 'Unnamed' in i]+['msno'],axis=1).to_numpy()\n",
    "    last_15_days_log = pd.read_csv(\"last_15days_log_{}.csv\".format(name)).drop([i for i in pd.read_csv(\"last_15days_log_{}.csv\".format(name)).columns if 'Unnamed' in i]+['msno'],axis=1).to_numpy()\n",
    "    last_month_log = pd.read_csv(\"last_month_log_{}.csv\".format(name)).drop([i for i in pd.read_csv(\"last_month_log_{}.csv\".format(name)).columns if 'Unnamed' in i]+['msno'],axis=1).to_numpy()\n",
    "    all_month_log = pd.read_csv(\"all_log_{}.csv\".format(name)).drop([i for i in pd.read_csv(\"all_log_{}.csv\".format(name)).columns if 'Unnamed' in i]+['msno'],axis=1).to_numpy()\n",
    "    last_month_15days_subtract_log = last_month_log-last_15_days_log\n",
    "    #stack all the preprocessed fetures of log \n",
    "    user_log=np.hstack((final_day_log,last_month_15days_subtract_log,last_15_days_log,last_month_log,all_month_log))\n",
    "    #save the file for later use\n",
    "    np.save(\"log_{}\".format(name),user_log)\n",
    "    print(\"user log\",user_log.shape)\n",
    "    #members file details\n",
    "    members=np.load('{}_member.npy'.format(name))\n",
    "    print(\"members\",members.shape)\n",
    "    #load all the features generated using transaction file\n",
    "    total_churn_notchurn_count = np.load('total_churn_or_not_count_{}.npy'.format(name))\n",
    "    last_5_churn_notchurns     = np.load('Last_5_churn_or_not_{}.npy'.format(name))\n",
    "    total_cancel_feature       = np.load('{}_total_num_cancel.npy'.format(name))\n",
    "    final_trans                = pd.read_csv('{}_final_transact.csv'.format(name))\n",
    "    last_before_trans          = pd.read_csv('{}_last_before_transact_features_msno_order'.format(name))\n",
    "    trans_sum_count_avg        = np.load('transact_sum_count_avg_features_{}.npy'.format(name))\n",
    "    Last_5_actual_amount_paid  = np.load('Last_5_actual_amount_paid_{}.npy'.format(name))\n",
    "    Last_5_discount            = np.load('Last_5_discount_{}.npy'.format(name))\n",
    "    Last_5_payment_plan_days   = np.load('Last_5_payment_plan_days_{}.npy'.format(name))\n",
    "    Last_5_per_day_price       = np.load('Last_5_per_day_price_{}.npy'.format(name))\n",
    "    Final_Add                  = np.load('Final_Add_{}.npy'.format(name))\n",
    "    #remove label feature and unnamed columns i,e., index\n",
    "    last_before_trans = last_before_trans.drop([i for i in last_before_trans.columns if 'is_churn' in i or 'Unnamed' in i],axis=1)\n",
    "    #to avoid over fitting converting a date feature into a number of days from last expire date\n",
    "    last_before_trans['Last_before_membership_expire_date'] = pd.to_datetime(last_before_trans['Last_before_membership_expire_date'].astype(str),format='%Y%m%d')\n",
    "    last_before_trans['Last_before_membership_expire_date'] = pd.to_datetime(date[0],format='%Y%m%d')-last_before_trans['Last_before_membership_expire_date']\n",
    "    last_before_trans['Last_before_membership_expire_date'] = last_before_trans['Last_before_membership_expire_date']/np.timedelta64(1, 'D')\n",
    "    last_before_trans['Last_before_transaction_date'] = pd.to_datetime(last_before_trans['Last_before_transaction_date'].astype(str),format='%Y%m%d')\n",
    "    last_before_trans['Last_before_transaction_date'] = pd.to_datetime(date[0],format='%Y%m%d')-last_before_trans['Last_before_transaction_date']\n",
    "    last_before_trans['Last_before_transaction_date'] = last_before_trans['Last_before_transaction_date']/np.timedelta64(1, 'D')\n",
    "    #count vectorizing a payment feature\n",
    "    final_trans = final_trans.drop(['Unnamed: 0','transaction_date','membership_expire_date'],axis=1)\n",
    "    #Last Before Count Vectorizer\n",
    "    if name =='train':\n",
    "        #https://stackoverflow.com/a/33264704\n",
    "        payment_method_id = CountVectorizer( lowercase=False, binary=True, token_pattern = r\"(?u)\\b\\w+\\b\")\n",
    "        payment_method_id.fit(list(map(str, last_before_trans['Last_before_payment_method_id'].values.astype(int))))\n",
    "        auto_renew = CountVectorizer( lowercase=False, binary=True, token_pattern = r\"(?u)\\b\\w+\\b\")\n",
    "        auto_renew.fit(list(map(str, last_before_trans['Last_before_is_auto_renew'].values.astype(int))))\n",
    "        is_cancel = CountVectorizer( lowercase=False, binary=True, token_pattern = r\"(?u)\\b\\w+\\b\")\n",
    "        is_cancel.fit(list(map(str, last_before_trans['Last_before_is_cancel'].values.astype(int))))\n",
    "        pickle.dump((payment_method_id,auto_renew,is_cancel),open(\"last_before_trans.pickle\",\"wb\"))\n",
    "        last_before_payment_method = payment_method_id.transform(list(map(str,last_before_trans['Last_before_payment_method_id'].values.astype(int))))\n",
    "        last_before_auto_renew = auto_renew.transform(list(map(str,last_before_trans['Last_before_is_auto_renew'].values.astype(int))))\n",
    "        last_before_cancel =is_cancel.transform(list(map(str,last_before_trans['Last_before_is_cancel'].values.astype(int))))\n",
    "    else:\n",
    "        payment_method_id,auto_renew,is_cancel = pickle.load(open(\"last_before_trans.pickle\",\"rb\"))\n",
    "        last_before_payment_method = payment_method_id.transform(list(map(str,last_before_trans['Last_before_payment_method_id'].values.astype(int))))\n",
    "        last_before_auto_renew = auto_renew.transform(list(map(str,last_before_trans['Last_before_is_auto_renew'].values.astype(int))))\n",
    "        last_before_cancel =is_cancel.transform(list(map(str,last_before_trans['Last_before_is_cancel'].values.astype(int))))\n",
    "    last_before_trans = last_before_trans.drop(['Last_before_payment_method_id','Last_before_is_auto_renew','Last_before_is_cancel'],axis=1)\n",
    "    last_before_trans = np.array(last_before_trans)\n",
    "    last_before_trans = np.hstack((last_before_payment_method.todense(),last_before_auto_renew.todense(),last_before_trans,last_before_cancel.todense()))\n",
    "    # Count vectorizing a final day payemnt id feature\n",
    "    #Final Count Vectorizer\n",
    "    if name =='train':\n",
    "        #https://stackoverflow.com/a/33264704\n",
    "        fpayment_method_id = CountVectorizer( lowercase=False, binary=True, token_pattern = r\"(?u)\\b\\w+\\b\")\n",
    "        val = list(final_trans['payment_method_id'])\n",
    "        val[0] = np.nan\n",
    "        fpayment_method_id.fit(list(map(str, np.array(val).astype(int))))\n",
    "        fauto_renew = CountVectorizer( lowercase=False, binary=True, token_pattern = r\"(?u)\\b\\w+\\b\")\n",
    "        val = list(final_trans['is_auto_renew'])\n",
    "        val[0] = np.nan\n",
    "        fauto_renew.fit(list(map(str, np.array(val).astype(int))))\n",
    "        fis_cancel = CountVectorizer( lowercase=False, binary=True, token_pattern = r\"(?u)\\b\\w+\\b\")\n",
    "        val = list(final_trans['is_cancel'])\n",
    "        val[0] = np.nan\n",
    "        fis_cancel.fit(list(map(str, np.array(val).astype(int))))\n",
    "        pickle.dump((fpayment_method_id,fauto_renew,fis_cancel),open(\"final_trans.pickle\",\"wb\"))\n",
    "        payment_method = fpayment_method_id.transform(list(map(str,final_trans['payment_method_id'].values.astype(int))))\n",
    "        auto_renew = fauto_renew.transform(list(map(str,final_trans['is_auto_renew'].values.astype(int))))\n",
    "        cancel = fis_cancel.transform(list(map(str,final_trans['is_cancel'].values.astype(int))))\n",
    "    else:\n",
    "        fpayment_method_id,fauto_renew,fis_cancel = pickle.load(open(\"final_trans.pickle\",\"rb\"))\n",
    "        payment_method = fpayment_method_id.transform(list(map(str,final_trans['payment_method_id'].values.astype(int))))\n",
    "        auto_renew = fauto_renew.transform(list(map(str,final_trans['is_auto_renew'].values.astype(int))))\n",
    "        cancel = fis_cancel.transform(list(map(str,final_trans['is_cancel'].values.astype(int))))\n",
    "    final_trans = final_trans.drop(['msno','payment_method_id','is_auto_renew','is_cancel'],axis=1)\n",
    "    final_trans = np.array(final_trans)\n",
    "    final_trans = np.hstack((payment_method.todense(),auto_renew.todense(),cancel.todense(),final_trans))\n",
    "    #churn rate feature\n",
    "    churn_rate=total_churn_notchurn_count[:,0]/(total_churn_notchurn_count[:,0]+total_churn_notchurn_count[:,1])\n",
    "    trans = np.hstack((Final_Add,trans_sum_count_avg,Last_5_actual_amount_paid,Last_5_discount,Last_5_payment_plan_days,Last_5_per_day_price,\n",
    "                          total_cancel_feature.reshape(-1,1),last_5_churn_notchurns,total_churn_notchurn_count,last_before_trans,final_trans,churn_rate.reshape(-1,1)))\n",
    "    np.save(\"trans_{}\".format(name),trans)\n",
    "    print(\"trans\",trans.shape)\n",
    "    #combine all the features\n",
    "    X = np.hstack((user_log,members,trans))\n",
    "    #convert its type to float to input nan\n",
    "    X = X.astype(np.float64)\n",
    "    #replace NaN with -1.0\n",
    "    X[np.isnan(X)]=-1.0\n",
    "    np.save('X_{}'.format(name),X) \n",
    "    print(\"X_{}\".format(name),X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [10:52<00:00, 59.35s/it]\n"
     ]
    }
   ],
   "source": [
    "# msno = pd.read_csv(\"user_label_201702.csv\")\n",
    "# msno[msno.msno=='+2vC1rM36Emx77UanRb3cUohWiIf7knfVIDO2+R78BE='].to_csv(\"query_msno.csv\")\n",
    "# t = pd.read_csv(\"transactions.csv\")\n",
    "# trans = t[t.msno==\"+2vC1rM36Emx77UanRb3cUohWiIf7knfVIDO2+R78BE=\"]\n",
    "# trans.to_csv(\"query_trans.csv\",index=False)\n",
    "# m = pd.read_csv(\"members_v3.csv\")\n",
    "# m = m[m.msno==\"+2vC1rM36Emx77UanRb3cUohWiIf7knfVIDO2+R78BE=\"]\n",
    "# m.to_csv(\"query_member.csv\",index=False)\n",
    "# f = [\"./chunk/user_logs_chunk_{}.csv\".format(i) for i in range(10)]+[\"./data/churn_comp_refresh/user_logs_v2.csv\"]\n",
    "# l_temp = []\n",
    "# for i in tqdm(f):\n",
    "#     temp = pd.read_csv(i)\n",
    "#     l_temp.append(temp[temp.msno=='+2vC1rM36Emx77UanRb3cUohWiIf7knfVIDO2+R78BE='])\n",
    "# l = pd.concat(l_temp)\n",
    "# l\n",
    "# l.to_csv(\"query_log.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "#https://www.kaggle.com/c/bioresponse/discussion/1831\n",
    "def log_loss(predicted, target):\n",
    "    if len(predicted) != len(target):\n",
    "        print('lengths not equal!')\n",
    "        return\n",
    "    target = [float(x) for x in target]   # make sure all float values\n",
    "    predicted = [min([max([x,1e-15]),1-1e-15]) for x in predicted]  # within (0,1) interval\n",
    "    return -(1.0/len(target))*sum([target[i]*log(predicted[i]) +(1.0-target[i])*log(1.0-predicted[i]) for i in range(len(target))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Final_fun_1(log_files,member_file,trans_files,msno_file,last_date,last_month,days,date):\n",
    "    name = 'query'\n",
    "    msno_df = pd.read_csv(msno_file)\n",
    "    member_df =  pd.read_csv(member_file)\n",
    "    Train_ = preprocess_member(member_df,msno_df)\n",
    "    np.save(\"{}_member\".format(name),Train_)\n",
    "    preprocess_log(log_files,msno_file,name,last_date,last_month,days)\n",
    "    Transaction_preprocess(trans_files,msno_file,last_date,name,date)\n",
    "    prepare_data('query',date)\n",
    "    query_data = np.load(\"X_query.npy\")\n",
    "    lgb_model= lgb.Booster(model_file='best model')\n",
    "    lgb_pred = lgb_model.predict(query_data)\n",
    "    query = pd.read_csv(msno_file)[['msno','is_churn']]\n",
    "    query['is_churn'] = lgb_pred.clip(0.+1e-15, 1-1e-15) \n",
    "    query[['msno','is_churn']].to_csv('lgb_result.csv', index = False)\n",
    "    return query[['msno','is_churn']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Final_fun_2():\n",
    "    print(\"Enter the Log files name:\")\n",
    "    log_files = list(i.strip() for i in input().split())\n",
    "    print(\"Enter the msno files name:\")\n",
    "    msno_file = input()\n",
    "    print(\"Enter the member files name:\")\n",
    "    member_file = input()\n",
    "    print(\"Enter the last date of transaction:\")\n",
    "    last_date = int(input())\n",
    "    print(\"Enter the last month of transaction:\")\n",
    "    last_month = tuple(int(i.strip()) for i in input().split(\",\"))\n",
    "    print(\"Enter the last 15 days of transaction:\")\n",
    "    days = tuple(int(i.strip()) for i in input().split(\",\"))\n",
    "    print(\"Enter the transaction files name:\")\n",
    "    trans_files = list(i.strip() for i in input().split())\n",
    "    print(\"Enter the membership expirationdate:\")\n",
    "    date = tuple(int(i.strip()) for i in input().split(\",\"))\n",
    "    y_ = Final_fun_1(log_files,member_file,trans_files,msno_file,last_date,last_month,days,date)\n",
    "    y_pred = y_['is_churn']\n",
    "    yes = input(\"Do you want to calculate log loss?[y/n]:\")\n",
    "    if 'y' in yes.lower():\n",
    "        y = pd.read_csv(msno_file).is_churn.astype(int)            \n",
    "        return log_loss(y_pred,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the Log files name:\n",
      "query_log.csv\n",
      "Enter the msno files name:\n",
      "query_msno.csv\n",
      "Enter the member files name:\n",
      "query_member.csv\n",
      "Enter the last date of transaction:\n",
      "20170131\n",
      "Enter the last month of transaction:\n",
      "20170101, 20170131\n",
      "Enter the last 15 days of transaction:\n",
      "20170116, 20170131\n",
      "Enter the transaction files name:\n",
      "query_trans.csv\n",
      "Enter the membership expirationdate:\n",
      "20170201,20170228\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00,  6.20it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00,  6.05it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00,  6.02it/s]\n",
      "100%|██████████| 7/7 [00:01<00:00,  6.40it/s]\n",
      "100%|██████████| 24/24 [00:00<00:00, 29808.50it/s]\n",
      "100%|██████████| 25/25 [00:00<00:00, 40376.43it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1448.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. total number of cancel feature\n",
      "2. Last Before transaction features\n",
      "3. Final transaction feature\n",
      " Last 5 Churn and total churn and not churn rate\n",
      "4. Total_churn_and_not_churn_count:\n",
      "5. Last_5_not_churns:\n",
      "additinal feature genration\n",
      "6. Discount feature \n",
      "7. pay per day\n",
      "8. Sum_count_and_average_features:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1/1 [00:00<00:00, 2184.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9. slast five features\n",
      "10. Final Transaction Additional Features\n",
      "11. number of days from transaction date to membership expire date\n",
      "12. Days_since_final_transact:\n",
      "13. Days left\n",
      "14. Payment_plan_days_subract_interval:\n",
      "user log (1, 88)\n",
      "members (1, 33)\n",
      "trans (1, 135)\n",
      "X_query (1, 256)\n",
      "Do you want to calculate log loss?[y/n]:y\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.009527622681534325"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Final_fun_2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input required for above final method\n",
    "#     log_files = [\"query_log.csv\"]\n",
    "#     msno_file = \"query_msno.csv\"\n",
    "#     member_file = \"query_member.csv\"\n",
    "#     last_date = 20170131\n",
    "#     last_month = (20170101, 20170131)\n",
    "#     days = (20170116, 20170131)\n",
    "#     trans_files = [\"query_trans.csv\"]\n",
    "#     date = (20170201,20170228)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
